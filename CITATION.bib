% Active Statistical Inference reference
@InProceedings{zrnic2024active,
  title = 	 {Active Statistical Inference},
  author =       {Zrnic, Tijana and Candes, Emmanuel},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {62993--63010},
  year = 	 {2024},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zrnic24a/zrnic24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zrnic24a.html},
  abstract = 	 {Inspired by the concept of active learning, we propose active inference—a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful tests. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.}
}

% Balanced sampling reference
@article{deville2004efficient ,
    author = {Deville, Jean-Claude and Tillé, Yves},
    title = {Efficient balanced sampling: The cube method},
    journal = {Biometrika},
    volume = {91},
    number = {4},
    pages = {893-912},
    year = {2004},
    abstract = {A balanced sampling design is defined by the property that the Horvitz–Thompson estimators of the population totals of a set of auxiliary variables equal the known totals of these variables. Therefore the variances of estimators of totals of all the variables of interest are reduced, depending on the correlations of these variables with the controlled variables. In this paper, we develop a general method, called the cube method, for selecting approximately balanced samples with equal or unequal inclusion probabilities and any number of auxiliary variables.},
    issn = {0006-3444},
    doi = {10.1093/biomet/91.4.893},
    url = {https://doi.org/10.1093/biomet/91.4.893},
    eprint = {https://academic.oup.com/biomet/article-pdf/91/4/893/1249543/914893.pdf},
}

@article{tille2011ten,
  title={Ten years of balanced sampling with the cube method: An appraisal},
  author={Till{\'e}, Yves},
  journal={Survey methodology},
  volume={37},
  number={2},
  pages={215--226},
  year={2011}
}

% XGBoost reference
@inproceedings{chen2016xgboost,
author = {Chen, Tianqi and Guestrin, Carlos},
title = {XGBoost: A Scalable Tree Boosting System},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939785},
doi = {10.1145/2939672.2939785},
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {785–794},
numpages = {10},
keywords = {large-scale machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}


% Dataset reference
@article{friedman1991multivariate,
  title={Multivariate adaptive regression splines},
  author={Friedman, Jerome H},
  journal={Annals of Statistics},
  volume={19},
  number={1},
  pages={1--67},
  year={1991}
}

@misc{bike_sharing_275,
  author       = {Fanaee-T, Hadi},
  title        = {{Bike Sharing}},
  year         = {2013},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5W894}
}

@misc{communities_and_crime_183,
  author       = {Redmond, Michael},
  title        = {{Communities and Crime}},
  year         = {2002},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C53W3X}
}

@misc{concrete_compressive_strength_165,
  author       = {Yeh, I-Cheng},
  title        = {{Concrete Compressive Strength}},
  year         = {1998},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PK67}
}

@misc{energy_efficiency_242,
  author       = {Tsanas, Athanasios and Xifara, Angeliki},
  title        = {{Energy Efficiency}},
  year         = {2012},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C51307}
}

@misc{superconductivty_data_464,
  author       = {Hamidieh, Kam},
  title        = {{Superconductivty Data}},
  year         = {2018},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C53P47}
}

@misc{life_expectancy_kaggle,
  author       = {Kumar, Abhinav},
  title        = {Life Expectancy ({WHO})},
  year         = {2020},
  howpublished = {Kaggle},
  url          = {https://www.kaggle.com/datasets/...},
  note         = {https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who}
}

@misc{credit_fraud_kaggle,
  author       = {Lopez, Emmanuel},
  title        = {Credit Card Fraud Detection},
  year         = {2019},
  howpublished = {Kaggle},
  url          = {https://www.kaggle.com/datasets/...},
  note         = {https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}
}

@misc{post,
  author       = {Pew},
  title        = {American trends panel (ATP) wave 79},
  year         = {2020},
  url          = {https://www.pewresearch.org/science/dataset/american-trends-panel-wave-79/},
  note         = {https://www.pewresearch.org/science/dataset/american-trends-panel-wave-79/}
}
